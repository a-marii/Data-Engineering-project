# Data-Engineering-project

The computational experiments conducted on the Spark-based distributed system designed for handling large volumes of Reddit data yielded valuable insights into its scalability and performance characteristics. 

For data preprocessing, two methods were employed: removing the stopwords in each Reddit category and performing n-gram analysis. The *ml.feature* package of PySpark was utilized to implement the aforementioned methods. Additionally, aggregation and transformation operators were employed in the analysis process. At the outset of the process, the data underwent cleaning to remove punctuation characters, numbers, and white spaces. Subsequently, the *Tokenizer* function was applied to segment each data point into separate words. To eliminate stop words, the *StopWordsRemover* function was utilized. Following this, the data was grouped into categories, and the number of important words (not stop words) in each Reddit category was computed. The most popular category was *askreddit* with more than 50\% of stopwords. Next, an n-gram analysis was conducted with the *n=2* and *n=3, enabling the identification of the most common pairs and triples of words in sentences. The *NGram* function was used for this purpose. It was discovered that the most common pair is *'of the'*, and the most frequent sequence of three words is *'a lot of'*.

Vertical scaling experiments, which involved increasing the computational resources of individual nodes, demonstrated consistent performance improvements across various processing tasks, albeit with some deviations in certain steps like data frame processing and n-gram analysis. Horizontal scaling experiments, focusing on adding more worker nodes to the Spark cluster, showcased promising scalability, with notable performance gains observed across most processing steps. However, the scalability wasn't entirely linear, indicating potential areas for optimization and resource utilization improvements.

One notable observation was the storage constraints faced during the 3-word n-gram analysis, where the exponential increase in possible combinations significantly increased storage requirements, potentially leading to I/O bottlenecks. This highlights the importance of considering both computational and storage aspects when designing and scaling distributed systems.

Overall, the chosen approach of leveraging Hadoop for distributed file storage and Spark for data processing proved effective in achieving the project objectives. The experiments provided valuable insights into the system's performance characteristics and scalability, laying a solid foundation for further optimization and refinement. Additionally, the experiences gained through this project, including deploying services on a cloud platform and working with Spark APIs, have been invaluable for the team's skill development and learning journey.